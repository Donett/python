线性回归：（最小二乘法的由来）

拟合平面函数->误差函数->最大似然函数->对数似然->最小二乘法
目标函数偏导->求出θ
评估方法R2≈1好
对似然函数的理解：

在已有预测数据的前提下，去计算他发生的概率。

误差函数正态分布概率已有，把他代入这个，求出概率

越大，预测数据发生的可能性越高，参数越合理



常说的概率是指给定参数后，预测即将发生的事件的可能性。拿硬币这个例子来说，我们已知一枚均匀硬币的正反面概率分别是0.5，要预测抛两次硬币，硬币都朝上的概率：

H代表Head，表示头朝上

p(HH | pH = 0.5) = 0.5*0.5 = 0.25.

这种写法其实有点误导，后面的这个p其实是作为参数存在的，而不是一个随机变量，因此不能算作是条件概率，更靠谱的写法应该是 p(HH;p=0.5)。

而似然概率正好与这个过程相反，我们关注的量不再是事件的发生概率，而是已知发生了某些事件，我们希望知道参数应该是多少。

现在我们已经抛了两次硬币，并且知道了结果是两次头朝上，这时候，我希望知道这枚硬币抛出去正面朝上的概率为0.5的概率是多少？正面朝上的概率为0.8的概率是多少？

如果我们希望知道正面朝上概率为0.5的概率，这个东西就叫做似然函数，可以说成是对某一个参数的猜想（p=0.5）



最大似然概率，就是在已知观测的数据的前提下，找到使得似然概率最大的参数值。

这就不难理解，在data mining领域，许多求参数的方法最终都归结为最大化似然概率的问题。


为什么引入似然函数？

似然函数取得最大值表示相应的参数能够使得统计模型最为合理。

似然函数的主要用法在于比较它相对取值，虽然这个数值本身不具备任何含义。

似然函数：让预测值成为真实值的可能性越大越好

似然函数类似概率,越大，落在正态分布中间段的概率越大，越接近预测值

为什么要log（似然函数）？

加法容易计算

为什么让后面结果越小越好？即为什么要最小？二乘法？不是最大二乘法？

推导化简的结果，使似然函数最大，即让后面的最小




1、拟合平面函数：



有几个特征，就有几个θ参数 

2、误差函数：



假设 误差是 独立，同分布，服从高斯分布的，在这个基础上做出结果，是符合要求的，所以可以接受这个假设



3.似然函数->对数似然

似然函数越大越好





4.最小二乘法



5.目标函数最小-->偏导数=0

转化为矩阵运算

假设为凸函数：凸优化：偏导数=0，一般认为是极值点





机器学习一般是优化的思想，能直接求解出来还是比较少的

线性回归是巧合，能求出来



6.评估方法R2

